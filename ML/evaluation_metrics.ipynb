{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11429f16",
   "metadata": {},
   "source": [
    "# Evaluation Metrics in Machine Learning\n",
    "- the real way to measure how good your model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445404f",
   "metadata": {},
   "source": [
    "##  1ï¸âƒ£ Accuracy\n",
    "\n",
    "### What is Accuracy?\n",
    "\n",
    "Accuracy tells you **how often your model is correct**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0458f",
   "metadata": {},
   "source": [
    "**Accuracy= TotalÂ Predictions/CorrectÂ Predictions**\n",
    "\tâ€‹\n",
    "\n",
    "| Actual   | Predicted    | Result |\n",
    "|--------  |------------  |------- |\n",
    "| Spam     | Spam         | âœ…    |\n",
    "| Not Spam | Not Spam     | âœ…    |\n",
    "| Spam     | Not Spam     | âŒ    |\n",
    "| Not Spam | Not Spam     | âœ…    |\n",
    "| Spam     | Spam         | âœ…    |\n",
    "\n",
    "âœ… Correct Predictions = 4  \n",
    "âŒ Incorrect Predictions = 1  \n",
    "\n",
    "**Accuracy=5/4â€‹=0.8=80%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c127f",
   "metadata": {},
   "source": [
    "### Why it matters:\n",
    "- Gives you a **quick overall idea** of performance.  \n",
    "- But it can be **misleading** if your data is **imbalanced**.\n",
    "\n",
    "###  Example of Imbalance\n",
    "\n",
    "Suppose 95% of emails are **Not Spam** and only 5% are **Spam**.  \n",
    "If your model **always predicts \"Not Spam\"** â†’  \n",
    "âœ… 95% Accurate  \n",
    "âŒ 0% useful at detecting spam!\n",
    "\n",
    "Thatâ€™s why we need other metrics like **Precision**, **Recall**, and **F1-Score**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6aee2",
   "metadata": {},
   "source": [
    "##  2ï¸âƒ£ Precision\n",
    "\n",
    "###  What is Precision?\n",
    "\n",
    "Precision answers:  \n",
    "> â€œOut of all the positive predictions, how many were actually correct?â€\n",
    "\n",
    "**Precision=TrueÂ Positives/(TrueÂ Positives +Â FalseÂ Positivesâ€‹)**\n",
    "\n",
    "### ğŸ’¡ Example\n",
    "\n",
    "Your spam filter predicted 10 emails as spam:\n",
    "- 7 were actually spam âœ…  \n",
    "- 3 were not spam âŒ\n",
    "\n",
    "**Precision=7/(7+3)â€‹=0.7=70%**\n",
    "\n",
    "### Why it matters:\n",
    "Precision focuses on **how reliable your positive predictions are**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae93bbb",
   "metadata": {},
   "source": [
    "##  3ï¸âƒ£ Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "###  What is Recall?\n",
    "\n",
    "Recall answers:  \n",
    "> â€œOut of all the actual positive cases, how many did the model correctly identify?â€\n",
    "\n",
    "**Recall = TrueÂ Positives/(TrueÂ PositivesÂ +Â FalseÂ Negatives)**\n",
    "\t\n",
    "### Example\n",
    "\n",
    "- There are 10 actual spam emails.  \n",
    "- Your model correctly catches 7, but misses 3.\n",
    "**Recall=7/(7+3â€‹)=0.7=70%**\n",
    "\n",
    "### Why it matters:\n",
    "Recall focuses on **how many real positives your model captures**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5c601",
   "metadata": {},
   "source": [
    "## ğŸ§© 4ï¸âƒ£ F1-Score\n",
    "\n",
    "### ğŸ§  What is F1-Score?\n",
    "\n",
    "Itâ€™s the **harmonic mean** of precision and recall.\n",
    "\n",
    "F1 = 2Ã— PrecisionÂ Ã—Â Recall/\n",
    "        â€‹PrecisionÂ +Â Recall\n",
    "### Why use it:\n",
    "- Great for **imbalanced datasets**\n",
    "- Gives a **balanced measure** of model performance\n",
    "\n",
    "\n",
    "F1 rewards you only if youâ€™re **good at both** â€”  \n",
    "like a **balanced student**, not just strong in one subject."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

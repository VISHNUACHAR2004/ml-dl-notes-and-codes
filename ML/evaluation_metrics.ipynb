{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11429f16",
   "metadata": {},
   "source": [
    "# Evaluation Metrics in Machine Learning\n",
    "- the real way to measure how good your model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445404f",
   "metadata": {},
   "source": [
    "##  1️⃣ Accuracy\n",
    "\n",
    "### What is Accuracy?\n",
    "\n",
    "Accuracy tells you **how often your model is correct**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0458f",
   "metadata": {},
   "source": [
    "**Accuracy= Total Predictions/Correct Predictions**\n",
    "\t​\n",
    "\n",
    "| Actual   | Predicted    | Result |\n",
    "|--------  |------------  |------- |\n",
    "| Spam     | Spam         | ✅    |\n",
    "| Not Spam | Not Spam     | ✅    |\n",
    "| Spam     | Not Spam     | ❌    |\n",
    "| Not Spam | Not Spam     | ✅    |\n",
    "| Spam     | Spam         | ✅    |\n",
    "\n",
    "✅ Correct Predictions = 4  \n",
    "❌ Incorrect Predictions = 1  \n",
    "\n",
    "**Accuracy=5/4​=0.8=80%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c127f",
   "metadata": {},
   "source": [
    "### Why it matters:\n",
    "- Gives you a **quick overall idea** of performance.  \n",
    "- But it can be **misleading** if your data is **imbalanced**.\n",
    "\n",
    "###  Example of Imbalance\n",
    "\n",
    "Suppose 95% of emails are **Not Spam** and only 5% are **Spam**.  \n",
    "If your model **always predicts \"Not Spam\"** →  \n",
    "✅ 95% Accurate  \n",
    "❌ 0% useful at detecting spam!\n",
    "\n",
    "That’s why we need other metrics like **Precision**, **Recall**, and **F1-Score**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6aee2",
   "metadata": {},
   "source": [
    "##  2️⃣ Precision\n",
    "\n",
    "###  What is Precision?\n",
    "\n",
    "Precision answers:  \n",
    "> “Out of all the positive predictions, how many were actually correct?”\n",
    "\n",
    "**Precision=True Positives/(True Positives + False Positives​)**\n",
    "\n",
    "### 💡 Example\n",
    "\n",
    "Your spam filter predicted 10 emails as spam:\n",
    "- 7 were actually spam ✅  \n",
    "- 3 were not spam ❌\n",
    "\n",
    "**Precision=7/(7+3)​=0.7=70%**\n",
    "\n",
    "### Why it matters:\n",
    "Precision focuses on **how reliable your positive predictions are**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae93bbb",
   "metadata": {},
   "source": [
    "##  3️⃣ Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "###  What is Recall?\n",
    "\n",
    "Recall answers:  \n",
    "> “Out of all the actual positive cases, how many did the model correctly identify?”\n",
    "\n",
    "**Recall = True Positives/(True Positives + False Negatives)**\n",
    "\t\n",
    "### Example\n",
    "\n",
    "- There are 10 actual spam emails.  \n",
    "- Your model correctly catches 7, but misses 3.\n",
    "**Recall=7/(7+3​)=0.7=70%**\n",
    "\n",
    "### Why it matters:\n",
    "Recall focuses on **how many real positives your model captures**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5c601",
   "metadata": {},
   "source": [
    "## 🧩 4️⃣ F1-Score\n",
    "\n",
    "### 🧠 What is F1-Score?\n",
    "\n",
    "It’s the **harmonic mean** of precision and recall.\n",
    "\n",
    "F1 = 2× Precision × Recall/\n",
    "        ​Precision + Recall\n",
    "### Why use it:\n",
    "- Great for **imbalanced datasets**\n",
    "- Gives a **balanced measure** of model performance\n",
    "\n",
    "\n",
    "F1 rewards you only if you’re **good at both** —  \n",
    "like a **balanced student**, not just strong in one subject."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

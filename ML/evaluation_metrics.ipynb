{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11429f16",
   "metadata": {},
   "source": [
    "# Evaluation Metrics in Machine Learning\n",
    "- the real way to measure how good your model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445404f",
   "metadata": {},
   "source": [
    "##  1️⃣ Accuracy\n",
    "\n",
    "### What is Accuracy?\n",
    "\n",
    "Accuracy tells you **how often your model is correct**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0458f",
   "metadata": {},
   "source": [
    "**Accuracy= Total Predictions/Correct Predictions**\n",
    "\t​\n",
    "\n",
    "| Actual   | Predicted    | Result |\n",
    "|--------  |------------  |------- |\n",
    "| Spam     | Spam         | ✅    |\n",
    "| Not Spam | Not Spam     | ✅    |\n",
    "| Spam     | Not Spam     | ❌    |\n",
    "| Not Spam | Not Spam     | ✅    |\n",
    "| Spam     | Spam         | ✅    |\n",
    "\n",
    "✅ Correct Predictions = 4  \n",
    "❌ Incorrect Predictions = 1  \n",
    "\n",
    "**Accuracy=5/4​=0.8=80%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c127f",
   "metadata": {},
   "source": [
    "### Why it matters:\n",
    "- Gives you a **quick overall idea** of performance.  \n",
    "- But it can be **misleading** if your data is **imbalanced**.\n",
    "\n",
    "###  Example of Imbalance\n",
    "\n",
    "Suppose 95% of emails are **Not Spam** and only 5% are **Spam**.  \n",
    "If your model **always predicts \"Not Spam\"** →  \n",
    "✅ 95% Accurate  \n",
    "❌ 0% useful at detecting spam!\n",
    "\n",
    "That’s why we need other metrics like **Precision**, **Recall**, and **F1-Score**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6aee2",
   "metadata": {},
   "source": [
    "##  2️⃣ Precision\n",
    "\n",
    "###  What is Precision?\n",
    "\n",
    "Precision answers:  \n",
    "> “Out of all the positive predictions, how many were actually correct?”\n",
    "\n",
    "**Precision=True Positives/(True Positives + False Positives​)**\n",
    "\n",
    "### 💡 Example\n",
    "\n",
    "Your spam filter predicted 10 emails as spam:\n",
    "- 7 were actually spam ✅  \n",
    "- 3 were not spam ❌\n",
    "\n",
    "**Precision=7/(7+3)​=0.7=70%**\n",
    "\n",
    "### Why it matters:\n",
    "Precision focuses on **how reliable your positive predictions are**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae93bbb",
   "metadata": {},
   "source": [
    "##  3️⃣ Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "###  What is Recall?\n",
    "\n",
    "Recall answers:  \n",
    "> “Out of all the actual positive cases, how many did the model correctly identify?”\n",
    "\n",
    "**Recall = True Positives/(True Positives + False Negatives)**\n",
    "\t\n",
    "### Example\n",
    "\n",
    "- There are 10 actual spam emails.  \n",
    "- Your model correctly catches 7, but misses 3.\n",
    "**Recall=7/(7+3​)=0.7=70%**\n",
    "\n",
    "### Why it matters:\n",
    "Recall focuses on **how many real positives your model captures**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5c601",
   "metadata": {},
   "source": [
    "## 4️⃣ F1-Score\n",
    "\n",
    "###  What is F1-Score?\n",
    "\n",
    "It’s the **harmonic mean** of precision and recall.\n",
    "\n",
    "F1 = 2× Precision × Recall/\n",
    "        ​Precision + Recall\n",
    "### Why use it:\n",
    "- Great for **imbalanced datasets**\n",
    "- Gives a **balanced measure** of model performance\n",
    "\n",
    "\n",
    "F1 rewards you only if you’re **good at both** —  \n",
    "like a **balanced student**, not just strong in one subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987eac7a",
   "metadata": {},
   "source": [
    "## 5️⃣ Logarithmic Loss (Log Loss)\n",
    "\n",
    "###  What is Log Loss?\n",
    "\n",
    "Unlike accuracy or F1 (which only check correct vs incorrect),  \n",
    "**Log Loss measures how confident your predictions are** —  \n",
    "it’s used in **probabilistic models** like Logistic Regression.\n",
    "\n",
    "\n",
    "###  Example\n",
    "\n",
    "Suppose for one email:\n",
    "\n",
    "Actual: Spam (1)\n",
    "\n",
    "Predicted probability: 0.95 → great confidence ✅\n",
    "→ Log loss = small (good)\n",
    "\n",
    "If predicted 0.51 → still correct but uncertain 😬\n",
    "→ Log loss = higher\n",
    "\n",
    "If predicted 0.05 → confidently wrong ❌\n",
    "→ Log loss = huge (very bad)\n",
    "\n",
    "| Case  | Actual   | Predicted Probability  | Log Loss  | Interpretation          |\n",
    "|-------|--------- |------------------------|-----------|----------------         |\n",
    "| 1     | 1        | 0.95                   | Small     | ✅ Confident & Correct  |\n",
    "| 2     | 1        | 0.51                   | Moderate  | 😬 Uncertain            |\n",
    "| 3     | 1        | 0.05                   | Huge      | ❌ Confidently Wrong    |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

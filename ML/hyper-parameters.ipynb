{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee9e158",
   "metadata": {},
   "source": [
    "# âš™ï¸ Machine Learning Hyperparameters\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ Learning Rate (Î·)\n",
    "\n",
    "ğŸ”¹ **What:**\n",
    "\n",
    "Itâ€™s the step size that controls how much we update the modelâ€™s weights during training.  \n",
    "\n",
    "\n",
    "ğŸ”¹ **Why:**\n",
    "\n",
    "It decides how fast or slow your model â€œlearnsâ€.\n",
    "\n",
    "- If itâ€™s too high â†’ the model takes huge steps, jumps over the minimum (fails to converge).  \n",
    "- If itâ€™s too low â†’ the model learns very slowly, takes ages to reach the minimum.\n",
    "\n",
    "\n",
    "ğŸ”¹ **How:**\n",
    "\n",
    "You usually start with a small value like `0.01` or `0.001`, and sometimes use **Learning Rate Schedulers** (to decrease it over time).\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18031b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2192844",
   "metadata": {},
   "source": [
    "##  2ï¸âƒ£ Batch Size\n",
    "\n",
    "### ğŸ”¹ What:\n",
    "The number of training samples processed before the model updates its weights once.\n",
    "\n",
    "\n",
    "### ğŸ”¹ Why:\n",
    "To balance between **speed** and **accuracy**.\n",
    "\n",
    "- **Large batch size â†’** more accurate gradient (stable learning) but uses lots of memory.  \n",
    "- **Small batch size â†’** faster updates but noisy (less stable).\n",
    "\n",
    "\n",
    "### ğŸ”¹ How:\n",
    "You divide the dataset into **mini-batches** (e.g., 32, 64, 128).  \n",
    "Each mini-batch runs through the network â†’ loss computed â†’ weights updated.\n",
    "\n",
    "\n",
    "### ğŸ”¹ Example:\n",
    "```python\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efb31c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7357dc",
   "metadata": {},
   "source": [
    "##  3ï¸âƒ£ Number of Epochs\n",
    "\n",
    "### ğŸ”¹ What:\n",
    "One **epoch** = one full pass through the entire training dataset.\n",
    "\n",
    "\n",
    "### ğŸ”¹ Why:\n",
    "You rarely learn everything in one go.\n",
    "\n",
    "- **Too few epochs â†’** underfitting (model hasnâ€™t learned enough).  \n",
    "- **Too many epochs â†’** overfitting (model memorizes training data).\n",
    "\n",
    "\n",
    "### ğŸ”¹ How:\n",
    "You set it as a number â€” for example, `epochs=50` â€” and use **Early Stopping** to halt training if performance stops improving.\n",
    "\n",
    "\n",
    "### ğŸ”¹ Example:\n",
    "```python\n",
    "model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da8cf49",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Analogy:\n",
    "\n",
    "- Think of epochs as study revisions.\n",
    "- You read your textbook (dataset) once â†’ youâ€™ll forget things (underfit).\n",
    "- You reread it 100 times â†’ you memorize examples but forget concepts (overfit).\n",
    "- Reading it 5â€“10 times, grasping key ideas â†’ perfect balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3ae4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ae22a",
   "metadata": {},
   "source": [
    "##  4ï¸âƒ£ Optimizer\n",
    "\n",
    "### ğŸ”¹ What:\n",
    "Itâ€™s the **algorithm that adjusts the modelâ€™s weights** to minimize the loss function using gradients.\n",
    "\n",
    "\n",
    "### ğŸ”¹ Why:\n",
    "Because **raw gradient descent** is simple but inefficient.  \n",
    "Optimizers make learning **faster** and **more stable**.\n",
    "\n",
    "\n",
    "### ğŸ”¹ How:\n",
    "They modify how gradients update weights. Common optimizers include:\n",
    "\n",
    "- **SGD:** Basic gradient descent.  \n",
    "- **Momentum:** Adds â€œinertiaâ€ to smooth the path.  \n",
    "- **Adam:** Adaptive learning rate + momentum (**most used**).  \n",
    "- **RMSProp:** Similar to Adam, adjusts step size per parameter.\n",
    "\n",
    "\n",
    "### ğŸ”¹ Example:\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d90bed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd6194",
   "metadata": {},
   "source": [
    "##  5ï¸âƒ£ Loss Function\n",
    "\n",
    "### ğŸ”¹ What:\n",
    "A **mathematical function** that measures how far your modelâ€™s predictions are from the actual values.\n",
    "\n",
    "### ğŸ”¹ Why:\n",
    "Itâ€™s what the model tries to **minimize** during training.  \n",
    "Without it, the model has **no idea** if itâ€™s improving or not.\n",
    "\n",
    "### ğŸ”¹ How:\n",
    "The type of loss function depends on the **problem type**:\n",
    "\n",
    "####  Regression â†’ Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "####  Classification â†’ Cross Entropy / Log Loss\n",
    "\n",
    "### ğŸ”¹ Example:\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

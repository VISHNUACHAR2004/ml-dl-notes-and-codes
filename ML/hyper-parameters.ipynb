{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee9e158",
   "metadata": {},
   "source": [
    "# ⚙️ Machine Learning Hyperparameters\n",
    "\n",
    "\n",
    "## 1️⃣ Learning Rate (η)\n",
    "\n",
    "🔹 **What:**\n",
    "\n",
    "It’s the step size that controls how much we update the model’s weights during training.  \n",
    "\n",
    "\n",
    "🔹 **Why:**\n",
    "\n",
    "It decides how fast or slow your model “learns”.\n",
    "\n",
    "- If it’s too high → the model takes huge steps, jumps over the minimum (fails to converge).  \n",
    "- If it’s too low → the model learns very slowly, takes ages to reach the minimum.\n",
    "\n",
    "\n",
    "🔹 **How:**\n",
    "\n",
    "You usually start with a small value like `0.01` or `0.001`, and sometimes use **Learning Rate Schedulers** (to decrease it over time).\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18031b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2192844",
   "metadata": {},
   "source": [
    "##  2️⃣ Batch Size\n",
    "\n",
    "### 🔹 What:\n",
    "The number of training samples processed before the model updates its weights once.\n",
    "\n",
    "\n",
    "### 🔹 Why:\n",
    "To balance between **speed** and **accuracy**.\n",
    "\n",
    "- **Large batch size →** more accurate gradient (stable learning) but uses lots of memory.  \n",
    "- **Small batch size →** faster updates but noisy (less stable).\n",
    "\n",
    "\n",
    "### 🔹 How:\n",
    "You divide the dataset into **mini-batches** (e.g., 32, 64, 128).  \n",
    "Each mini-batch runs through the network → loss computed → weights updated.\n",
    "\n",
    "\n",
    "### 🔹 Example:\n",
    "```python\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efb31c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7357dc",
   "metadata": {},
   "source": [
    "##  3️⃣ Number of Epochs\n",
    "\n",
    "### 🔹 What:\n",
    "One **epoch** = one full pass through the entire training dataset.\n",
    "\n",
    "\n",
    "### 🔹 Why:\n",
    "You rarely learn everything in one go.\n",
    "\n",
    "- **Too few epochs →** underfitting (model hasn’t learned enough).  \n",
    "- **Too many epochs →** overfitting (model memorizes training data).\n",
    "\n",
    "\n",
    "### 🔹 How:\n",
    "You set it as a number — for example, `epochs=50` — and use **Early Stopping** to halt training if performance stops improving.\n",
    "\n",
    "\n",
    "### 🔹 Example:\n",
    "```python\n",
    "model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da8cf49",
   "metadata": {},
   "source": [
    "### 🔹 Analogy:\n",
    "\n",
    "- Think of epochs as study revisions.\n",
    "- You read your textbook (dataset) once → you’ll forget things (underfit).\n",
    "- You reread it 100 times → you memorize examples but forget concepts (overfit).\n",
    "- Reading it 5–10 times, grasping key ideas → perfect balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3ae4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ae22a",
   "metadata": {},
   "source": [
    "##  4️⃣ Optimizer\n",
    "\n",
    "### 🔹 What:\n",
    "It’s the **algorithm that adjusts the model’s weights** to minimize the loss function using gradients.\n",
    "\n",
    "\n",
    "### 🔹 Why:\n",
    "Because **raw gradient descent** is simple but inefficient.  \n",
    "Optimizers make learning **faster** and **more stable**.\n",
    "\n",
    "\n",
    "### 🔹 How:\n",
    "They modify how gradients update weights. Common optimizers include:\n",
    "\n",
    "- **SGD:** Basic gradient descent.  \n",
    "- **Momentum:** Adds “inertia” to smooth the path.  \n",
    "- **Adam:** Adaptive learning rate + momentum (**most used**).  \n",
    "- **RMSProp:** Similar to Adam, adjusts step size per parameter.\n",
    "\n",
    "\n",
    "### 🔹 Example:\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d90bed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd6194",
   "metadata": {},
   "source": [
    "##  5️⃣ Loss Function\n",
    "\n",
    "### 🔹 What:\n",
    "A **mathematical function** that measures how far your model’s predictions are from the actual values.\n",
    "\n",
    "### 🔹 Why:\n",
    "It’s what the model tries to **minimize** during training.  \n",
    "Without it, the model has **no idea** if it’s improving or not.\n",
    "\n",
    "### 🔹 How:\n",
    "The type of loss function depends on the **problem type**:\n",
    "\n",
    "####  Regression → Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "####  Classification → Cross Entropy / Log Loss\n",
    "\n",
    "### 🔹 Example:\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

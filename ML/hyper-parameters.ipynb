{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee9e158",
   "metadata": {},
   "source": [
    "# ⚙️ Machine Learning Hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Learning Rate (η)\n",
    "\n",
    "🔹 **What:**\n",
    "\n",
    "It’s the step size that controls how much we update the model’s weights during training.  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "🔹 **Why:**\n",
    "\n",
    "It decides how fast or slow your model “learns”.\n",
    "\n",
    "- If it’s too high → the model takes huge steps, jumps over the minimum (fails to converge).  \n",
    "- If it’s too low → the model learns very slowly, takes ages to reach the minimum.\n",
    "\n",
    "---\n",
    "\n",
    "🔹 **How:**\n",
    "\n",
    "You usually start with a small value like `0.01` or `0.001`, and sometimes use **Learning Rate Schedulers** (to decrease it over time).\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "optimizer = Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2192844",
   "metadata": {},
   "source": [
    "##  2️⃣ Batch Size\n",
    "\n",
    "### 🔹 What:\n",
    "The number of training samples processed before the model updates its weights once.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why:\n",
    "To balance between **speed** and **accuracy**.\n",
    "\n",
    "- **Large batch size →** more accurate gradient (stable learning) but uses lots of memory.  \n",
    "- **Small batch size →** faster updates but noisy (less stable).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 How:\n",
    "You divide the dataset into **mini-batches** (e.g., 32, 64, 128).  \n",
    "Each mini-batch runs through the network → loss computed → weights updated.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Example:\n",
    "```python\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1996041",
   "metadata": {},
   "source": [
    "# Logistic Regression — Step-by-Step Intuitive Explanation\n",
    "\n",
    "---\n",
    "\n",
    "##  What is Logistic Regression?\n",
    "\n",
    "Despite its name, **Logistic Regression is not used for regression** (predicting continuous numbers).  \n",
    "It’s actually a **classification algorithm** — used to predict **discrete categories (classes)** such as:\n",
    "\n",
    "- ✅ Yes / ❌ No  \n",
    "- 📧 Spam / Not Spam  \n",
    "- 🧬 Disease / No Disease  \n",
    "- 0 / 1  \n",
    "\n",
    "It predicts the **probability** that an input belongs to a particular class.\n",
    "\n",
    "\n",
    "We can’t fit a straight line like **Linear Regression** because the output isn’t continuous —  \n",
    "it’s **either 0 or 1**.\n",
    "\n",
    "\n",
    "## ⚙️ Why Logistic Regression?\n",
    "\n",
    "Because:\n",
    "- We want **probabilities**, not continuous predictions.  \n",
    "- It works perfectly when the output variable is **binary (0 or 1)**.  \n",
    "- It’s **simple, interpretable**, and often the **baseline model** for any classification task.\n",
    "\n",
    "\n",
    "## 📉 The Problem with Linear Regression\n",
    "\n",
    "If we used Linear Regression for this classification:\n",
    "\n",
    "We might get predictions like **-0.3, 0.5, 1.2, etc.**\n",
    "\n",
    "But probabilities can’t be **less than 0** or **greater than 1**.  \n",
    "\n",
    "So, we need a function that **squashes any real number into a range between 0 and 1**.\n",
    "\n",
    "\n",
    "##  The Solution — Sigmoid Function\n",
    "Logistic Regression uses the **Sigmoid Function** to convert any value into a probability.\n",
    "\n",
    "\n",
    "The sigmoid creates a smooth **S-shaped curve** (called the logistic curve).  \n",
    "It gradually transitions from 0 to 1 as x increases.\n",
    "\n",
    "\n",
    "## ⚙️ The Logistic Regression Equation\n",
    "\n",
    "\n",
    "\n",
    "Where:  \n",
    "- **p** → predicted probability that output = 1  \n",
    "- **mX + c** → same linear combination as in Linear Regression  \n",
    "- **σ** → sigmoid function applied to squash result into `[0, 1]`\n",
    "\n",
    "\n",
    "##  Understanding the Sigmoid Function\n",
    "\n",
    "1. The **sigmoid function** is essential in logistic regression — it converts raw model outputs (logits) into probabilities between **0 and 1**.  \n",
    "2. It forms an **\"S\"-shaped curve**, known as the **logistic curve**.  \n",
    "   - Because probabilities must lie between 0 and 1, this function is perfect for classification tasks.  \n",
    "3. We typically use a **threshold value of 0.5** to decide the class:\n",
    "   - If sigmoid output ≥ 0.5 → **Class 1**\n",
    "   - If sigmoid output < 0.5 → **Class 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52884928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensource",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e7a9d8",
   "metadata": {},
   "source": [
    "# Optimizers and types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea520931",
   "metadata": {},
   "source": [
    "### 1️⃣ WHAT is an Optimizer?\n",
    "\n",
    "**An optimizer is an algorithm that updates the model’s parameters (weights & biases) so that the model’s error (loss) is minimized.**\n",
    "Think of training as a journey down a hill:\n",
    "- The height of the hill = the loss (error)\n",
    "- The position on the hill = the model’s weights\n",
    "- The optimizer = the strategy you use to find the bottom (minimum error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1921d",
   "metadata": {},
   "source": [
    "### 2️⃣ WHY do we need Optimizers?\n",
    "\n",
    "**Because deep learning models have millions of weights, and we can’t manually adjust them.**\n",
    "\n",
    "We need a smart algorithm that can:\n",
    "\n",
    "- Learn which direction to move (increase or decrease weights)\n",
    "\n",
    "- Decide how big each step should be (learning rate)\n",
    "\n",
    "- Avoid overshooting the minimum\n",
    "\n",
    "- Handle complex “mountain landscapes” (local minima, saddle points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429be49",
   "metadata": {},
   "source": [
    "### Momentum Optimizer\n",
    "\n",
    "**What: Adds a fraction of the previous update to the current one — helps build speed in the right direction.**\n",
    "\n",
    "Why: Prevents oscillations and speeds up convergence.\n",
    "Analogy:\n",
    "Like pushing a heavy ball downhill — it may start slow, but momentum keeps it rolling past small bumps.\n",
    "\n",
    "\n",
    "### RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "**What: Adjusts learning rate for each weight based on how frequently it updates — slows down learning for frequent updates, speeds it up for rare ones.**\n",
    "**Why: Solves vanishing/exploding gradient issues (common in RNNs).**\n",
    "**How: Keeps a moving average of squared gradients.**\n",
    "\n",
    "Analogy:\n",
    "Imagine you’re walking downhill — if one direction keeps changing steeply, you take smaller steps there; if another is steady, you take longer steps.\n",
    "\n",
    "Example:\n",
    "Used heavily in RNNs and deep sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dab7eb",
   "metadata": {},
   "source": [
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**What: Combines Momentum + RMSProp**\n",
    "It keeps track of:\n",
    "\n",
    "- The average of gradients (like momentum)\n",
    "\n",
    "- The average of squared gradients (like RMSProp)\n",
    "\n",
    "**Why: Fastest and most widely used — adapts learning rate individually for each parameter.**\n",
    "\n",
    "Analogy:\n",
    "You’re a smart hiker — you remember the last few steps (momentum) and how rough the path was (RMSProp) to choose the best next move.\n",
    "\n",
    "Example:\n",
    "Used in almost all modern deep learning models — CNNs, RNNs, Transformers, GANs, BERT, GPT, etc.\n",
    "\n",
    "When: Default optimizer for most deep learning tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
